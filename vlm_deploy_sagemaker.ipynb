{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLM 部署 - SageMaker Endpoint\n",
    "\n",
    "本notebook提供完整的视觉语言模型(VLM)在SageMaker上的生产级部署流程，包括：\n",
    "- 模型准备和S3存储\n",
    "- LMI容器配置\n",
    "- 端点部署\n",
    "- 自动扩缩容配置\n",
    "- 推理调用示例\n",
    "\n",
    "## 前置要求\n",
    "- 在SageMaker Studio或Notebook Instance中运行\n",
    "- 确保有足够的ml.g6e实例配额\n",
    "- 准备好要部署的VLM模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查是否安装了依赖\n",
    "!pip list | grep -E \"(boto3|sagemaker|huggingface_hub|transformers)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按需安装缺失的的依赖，例如：\n",
    "!pip install boto3 sagemaker -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import json\n",
    "import base64\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model\n",
    "from datetime import datetime\n",
    "\n",
    "# 获取SageMaker session和权限\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sess._region_name\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "print(f\"Region: {region}, Role: {role}\")\n",
    "print(f\"Default bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 配置参数\n",
    "\n",
    "根据你的需求修改以下配置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 部署配置\n",
    "MODEL_S3_PATH = \"s3://your-bucket/models/qwen2-5-vl-7b/\"  # 修改为你的模型S3路径\n",
    "INSTANCE_TYPE = \"ml.g6e.2xlarge\"  # 实例类型\n",
    "INITIAL_INSTANCE_COUNT = 2  # 初始实例数量\n",
    "MODEL_NAME = \"qwen2-5-vl-7b\"  # 模型名称\n",
    "\n",
    "# 扩缩容配置\n",
    "MIN_CAPACITY = 2   # 最小实例数\n",
    "MAX_CAPACITY = 10  # 最大实例数\n",
    "TARGET_INVOCATIONS_PER_INSTANCE = 60  # 每实例每分钟调用数目标值\n",
    "TARGET_GPU_UTILIZATION = 70.0  # 目标GPU利用率\n",
    "TARGET_GPU_MEM_UTILIZATION = 85.0  # 目标GPU显存利用率\n",
    "\n",
    "# LMI容器配置\n",
    "MAX_MODEL_LEN = 4096  # 最大序列长度\n",
    "MAX_BATCH_SIZE = 32   # 最大批处理大小\n",
    "\n",
    "print(\"配置参数:\")\n",
    "print(f\"模型S3路径: {MODEL_S3_PATH}\")\n",
    "print(f\"实例类型: {INSTANCE_TYPE}\")\n",
    "print(f\"初始实例数: {INITIAL_INSTANCE_COUNT}\")\n",
    "print(f\"扩缩容范围: {MIN_CAPACITY}-{MAX_CAPACITY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 创建LMI配置\n",
    "\n",
    "创建serving.properties配置文件并打包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清理并创建配置目录\n",
    "!rm -rf lmi_config\n",
    "os.makedirs(\"lmi_config\", exist_ok=True)\n",
    "\n",
    "# 创建serving.properties配置\n",
    "serving_config = f\"\"\"engine=Python\n",
    "option.model_id={MODEL_S3_PATH}\n",
    "option.dtype=fp16\n",
    "option.rolling_batch=vllm\n",
    "option.tensor_parallel_degree=1\n",
    "option.device_map=auto\n",
    "option.max_model_len={MAX_MODEL_LEN}\n",
    "option.max_rolling_batch_size={MAX_BATCH_SIZE}\n",
    "option.use_v2_block_manager=true\n",
    "option.enable_streaming=false\n",
    "\"\"\"\n",
    "\n",
    "# 保存配置文件\n",
    "with open(\"lmi_config/serving.properties\", \"w\") as f:\n",
    "    f.write(serving_config)\n",
    "\n",
    "print(\"serving.properties内容:\")\n",
    "print(serving_config)\n",
    "\n",
    "# 打包配置\n",
    "with tarfile.open(\"lmi_config.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"lmi_config\", arcname=\"lmi_config\")\n",
    "\n",
    "print(\"\\n配置文件已打包为 lmi_config.tar.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 部署到SageMaker\n",
    "\n",
    "创建模型并部署到端点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用最新的LMI容器\n",
    "image_uri = f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.33.0-lmi15.0.0-cu128\"\n",
    "print(f\"使用LMI容器: {image_uri}\")\n",
    "\n",
    "# 上传配置包到S3\n",
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "code_artifact = sess.upload_data(\"lmi_config.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"配置包上传到: {code_artifact}\")\n",
    "\n",
    "# 创建模型\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "print(\"SageMaker模型已创建\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成端点名称\n",
    "endpoint_name = f\"vlm-{MODEL_NAME}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "print(f\"端点名称: {endpoint_name}\")\n",
    "\n",
    "# 部署端点 (异步)\n",
    "print(\"开始部署端点...\")\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=INITIAL_INSTANCE_COUNT,\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    endpoint_name=endpoint_name,\n",
    "    wait=False  # 异步部署，不等待完成\n",
    ")\n",
    "\n",
    "print(f\"✅ 端点部署已启动: {endpoint_name}\")\n",
    "print(\"⏱️ 预计部署时间: 10-15分钟\")\n",
    "\n",
    "# 保存端点名称供后续使用\n",
    "with open(\"endpoint_name.save\", \"w\") as f:\n",
    "    f.write(endpoint_name)\n",
    "    \n",
    "print(f\"\\n📍 端点名称已保存到 endpoint_name.save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 等待端点就绪\n",
    "\n",
    "**重要**: 必须等待端点状态变为 `InService` 后才能配置扩缩容！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# 创建SageMaker客户端\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "def check_endpoint_status(endpoint_name):\n",
    "    try:\n",
    "        response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        return response['EndpointStatus']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# 等待端点就绪\n",
    "print(f\"等待端点就绪: {endpoint_name}\")\n",
    "print(\"这可能需要10-15分钟...\")\n",
    "\n",
    "start_time = time.time()\n",
    "while True:\n",
    "    status = check_endpoint_status(endpoint_name)\n",
    "    elapsed = int(time.time() - start_time)\n",
    "    \n",
    "    print(f\"\\r⏱️  {elapsed//60:02d}:{elapsed%60:02d} - 状态: {status}\", end=\"\", flush=True)\n",
    "    \n",
    "    if status == \"InService\":\n",
    "        print(f\"\\n\\n✅ 端点已就绪! 总耗时: {elapsed//60}分{elapsed%60}秒\")\n",
    "        print(\"现在可以配置扩缩容了\")\n",
    "        break\n",
    "    elif status in [\"Failed\", \"OutOfService\"]:\n",
    "        print(f\"\\n\\n❌ 端点部署失败: {status}\")\n",
    "        # 获取失败原因\n",
    "        try:\n",
    "            response = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "            if 'FailureReason' in response:\n",
    "                print(f\"失败原因: {response['FailureReason']}\")\n",
    "        except:\n",
    "            pass\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)  # 每30秒检查一次\n",
    "    \n",
    "    # 超时检查 (30分钟)\n",
    "    if elapsed > 1800:\n",
    "        print(f\"\\n\\n⚠️  部署超时，请检查SageMaker控制台\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 配置弹性扩缩容\n",
    "\n",
    "**仅在端点状态为 InService 时执行此步骤！**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建Auto Scaling客户端\n",
    "autoscaling_client = boto3.client('application-autoscaling', region_name=region)\n",
    "\n",
    "# 注册扩缩容目标\n",
    "print(\"注册扩缩容目标...\")\n",
    "try:\n",
    "    autoscaling_client.register_scalable_target(\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=f'endpoint/{endpoint_name}/variant/AllTraffic',\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "        MinCapacity=MIN_CAPACITY,\n",
    "        MaxCapacity=MAX_CAPACITY\n",
    "    )\n",
    "    print(f\"✅ 扩缩容目标已注册: {MIN_CAPACITY}-{MAX_CAPACITY}个实例\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 注册扩缩容目标失败: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建扩缩容策略\n",
    "print(\"创建扩缩容策略...\")\n",
    "try:\n",
    "    # 方案1: 基于调用频率扩缩容 (当前使用)\n",
    "    autoscaling_client.put_scaling_policy(\n",
    "        PolicyName=f'vlm-scaling-policy-{MODEL_NAME}',\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=f'endpoint/{endpoint_name}/variant/AllTraffic',\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "        PolicyType='TargetTrackingScaling',\n",
    "        TargetTrackingScalingPolicyConfiguration={\n",
    "            'TargetValue': TARGET_INVOCATIONS_PER_INSTANCE,\n",
    "            'PredefinedMetricSpecification': {\n",
    "                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance'\n",
    "            },\n",
    "            'ScaleOutCooldown': 180,  # 3分钟快速扩容\n",
    "            'ScaleInCooldown': 300    # 5分钟谨慎缩容\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # 方案2: 基于GPU利用率扩缩容 (示例代码，未启用)\n",
    "    # autoscaling_client.put_scaling_policy(\n",
    "    #     PolicyName=f'vlm-gpu-scaling-{MODEL_NAME}',\n",
    "    #     ServiceNamespace='sagemaker',\n",
    "    #     ResourceId=f'endpoint/{endpoint_name}/variant/AllTraffic',\n",
    "    #     ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    #     PolicyType='TargetTrackingScaling',\n",
    "    #     TargetTrackingScalingPolicyConfiguration={\n",
    "    #         'TargetValue': TARGET_GPU_UTILIZATION,\n",
    "    #         'CustomizedMetricSpecification': {\n",
    "    #             'MetricName': 'GPUUtilization',\n",
    "    #             'Namespace': 'AWS/SageMaker',\n",
    "    #             'Dimensions': [\n",
    "    #                 {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "    #                 {'Name': 'VariantName', 'Value': 'AllTraffic'}\n",
    "    #             ],\n",
    "    #             'Statistic': 'Average'\n",
    "    #         },\n",
    "    #         'ScaleOutCooldown': 180,  # 3分钟快速扩容\n",
    "    #         'ScaleInCooldown': 300   # 5分钟谨慎缩容\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    # 方案3: 基于GPU内存利用率扩缩容 (示例代码，未启用)\n",
    "    # autoscaling_client.put_scaling_policy(\n",
    "    #     PolicyName=f'vlm-gpu-memory-scaling-{MODEL_NAME}',\n",
    "    #     ServiceNamespace='sagemaker',\n",
    "    #     ResourceId=f'endpoint/{endpoint_name}/variant/AllTraffic',\n",
    "    #     ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "    #     PolicyType='TargetTrackingScaling',\n",
    "    #     TargetTrackingScalingPolicyConfiguration={\n",
    "    #         'TargetValue': TARGET_GPU_MEM_UTILIZATION,\n",
    "    #         'CustomizedMetricSpecification': {\n",
    "    #             'MetricName': 'GPUMemoryUtilization',\n",
    "    #             'Namespace': 'AWS/SageMaker',\n",
    "    #             'Dimensions': [\n",
    "    #                 {'Name': 'EndpointName', 'Value': endpoint_name},\n",
    "    #                 {'Name': 'VariantName', 'Value': 'AllTraffic'}\n",
    "    #             ],\n",
    "    #             'Statistic': 'Average'\n",
    "    #         },\n",
    "    #         'ScaleOutCooldown': 120,  # GPU内存满时快速扩容\n",
    "    #         'ScaleInCooldown': 600   # 保守缩容避免频繁变化\n",
    "    #     }\n",
    "    # )\n",
    "\n",
    "    print(f\"✅ 扩缩容策略已创建\")\n",
    "    print(f\"   监控指标: 每实例调用数\")\n",
    "    print(f\"   目标阈值: {TARGET_INVOCATIONS_PER_INSTANCE}次/实例/分钟\")\n",
    "    print(f\"   扩容冷却: 3分钟\")\n",
    "    print(f\"   缩容冷却: 5分钟\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 创建扩缩容策略失败: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 推理调用示例\n",
    "\n",
    "端点就绪后，测试推理调用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建SageMaker Runtime客户端\n",
    "smr_client = boto3.client(\"sagemaker-runtime\", region_name=region)\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"将图像文件编码为base64字符串\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "def call_vlm_endpoint(endpoint_name, text_prompt, image_path=None, max_tokens=512):\n",
    "    \"\"\"调用VLM端点进行推理\"\"\"\n",
    "    \n",
    "    # 构建消息内容\n",
    "    content = [{\"type\": \"text\", \"text\": text_prompt}]\n",
    "    \n",
    "    # 如果有图像，添加图像内容\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        encoded_image = encode_image(image_path)\n",
    "        image_url = f\"data:image/png;base64,{encoded_image}\"\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_url}})\n",
    "    \n",
    "    # 构建请求\n",
    "    prompt = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": content}\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # 调用端点\n",
    "        response = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/json\",\n",
    "            Body=json.dumps(prompt),\n",
    "        )\n",
    "        \n",
    "        # 解析响应\n",
    "        result = json.loads(response[\"Body\"].read().decode(\"utf-8\"))\n",
    "        return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"调用失败: {e}\"\n",
    "\n",
    "print(\"推理调用函数已定义\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试文本推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试纯文本推理\n",
    "print(\"测试文本推理...\")\n",
    "text_prompt = \"请介绍一下人工智能的发展历史\"\n",
    "print(f\"\\n问题: {text_prompt}\")\n",
    "\n",
    "result = call_vlm_endpoint(endpoint_name, text_prompt)\n",
    "print(f\"\\n回答: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试视觉推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试视觉推理 (需要提供图像文件)\n",
    "image_path = \"test_image.jpg\"  # 修改为你的测试图像路径\n",
    "\n",
    "if os.path.exists(image_path):\n",
    "    print(f\"测试视觉推理: {image_path}\")\n",
    "    vision_prompt = \"请详细描述这张图片的内容\"\n",
    "    print(f\"\\n问题: {vision_prompt}\")\n",
    "    \n",
    "    result = call_vlm_endpoint(endpoint_name, vision_prompt, image_path)\n",
    "    print(f\"\\n回答: {result}\")\n",
    "else:\n",
    "    print(f\"测试图像不存在: {image_path}\")\n",
    "    print(\"请上传测试图像文件，或修改image_path变量\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 部署总结\n",
    "\n",
    "显示部署信息和后续步骤："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 VLM生产级部署完成!\")\n",
    "print(\"\\n📋 部署信息:\")\n",
    "print(f\"   端点名称: {endpoint_name}\")\n",
    "print(f\"   实例类型: {INSTANCE_TYPE}\")\n",
    "print(f\"   初始实例数: {INITIAL_INSTANCE_COUNT}\")\n",
    "print(f\"   扩缩容范围: {MIN_CAPACITY}-{MAX_CAPACITY}\")\n",
    "print(f\"   模型路径: {MODEL_S3_PATH}\")\n",
    "\n",
    "print(\"\\n🔧 已配置功能:\")\n",
    "print(\"   ✅ LMI容器部署\")\n",
    "print(\"   ✅ 自动扩缩容\")\n",
    "print(\"   ✅ 负载均衡\")\n",
    "print(\"   ✅ 推理调用\")\n",
    "\n",
    "print(\"\\n📊 监控建议:\")\n",
    "print(\"   - 在CloudWatch中监控GPUUtilization指标\")\n",
    "print(\"   - 设置ModelLatency告警 (建议<3秒)\")\n",
    "print(\"   - 监控InvocationsPerInstance指标\")\n",
    "\n",
    "print(\"\\n💰 成本优化:\")\n",
    "print(\"   - 考虑购买SageMaker Savings Plans (最高64%折扣)\")\n",
    "print(\"   - 根据实际使用情况调整扩缩容参数\")\n",
    "print(\"   - 启用AWQ量化减少显存占用\")\n",
    "\n",
    "print(\"\\n🔗 相关资源:\")\n",
    "print(f\"   - SageMaker控制台: https://console.aws.amazon.com/sagemaker/home?region={region}#/endpoints/{endpoint_name}\")\n",
    "print(f\"   - CloudWatch指标: https://console.aws.amazon.com/cloudwatch/home?region={region}#metricsV2:graph=~();search=SageMaker\")\n",
    "\n",
    "print(\"\\n⚠️  重要提醒:\")\n",
    "print(\"   - 不使用时请删除端点以避免产生费用\")\n",
    "print(\"   - 定期检查和优化扩缩容策略\")\n",
    "print(\"   - 建议在生产环境中启用VPC端点\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 清理资源 (可选)\n",
    "\n",
    "如果需要删除端点以停止计费："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取消注释以下代码来删除端点\n",
    "# 警告: 这将删除端点并停止服务\n",
    "\n",
    "# print(f\"删除端点: {endpoint_name}\")\n",
    "# try:\n",
    "#     sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "#     print(\"✅ 端点删除请求已提交\")\n",
    "# except Exception as e:\n",
    "#     print(f\"❌ 删除端点失败: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
